#!/usr/bin/env python
"""
train.py

Train a 2-layer LSTM + LayerNorm + MLP on the merged dataset generated by merge.py,
with support for:
  - Data parallelism via DistributedDataParallel (DDP)
  - Optional model sharding via FullyShardedDataParallel (FSDP)

Example (single node, 4 GPUs, DDP):

  torchrun --nproc_per_node=4 train.py \
    --merged_path Data/data/merged_lstm_dataset.parquet \
    --outdir runs/lstm_finbert_seq \
    --lookback 30 \
    --epochs 20 \
    --batch_size 64 \
    --dist_mode ddp \
    --amp

Example (single node, 4 GPUs, FSDP):

  torchrun --nproc_per_node=4 train.py \
    --merged_path Data/data/merged_lstm_dataset.parquet \
    --outdir runs/lstm_finbert_seq_fsdp \
    --lookback 30 \
    --epochs 20 \
    --batch_size 64 \
    --dist_mode fsdp \
    --amp
"""

import argparse
import os
import random
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import GradScaler, autocast

from model import SequenceConfig, NewsPriceSequenceDataset, LSTMRegressorWithLN


# ----------------- utilities -----------------

def set_seed(s: int = 1337):
    random.seed(s)
    np.random.seed(s)
    torch.manual_seed(s)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(s)


def ddp_setup(enable_distributed: bool):
    """
    Initialize torch.distributed if launched with torchrun and distributed is enabled.

    Returns:
        is_distributed: bool
        rank: int
        world_size: int
        is_main: bool
        device: torch.device
    """
    if enable_distributed and "RANK" in os.environ and "WORLD_SIZE" in os.environ:
        dist.init_process_group(backend="nccl")
        rank = int(os.environ["RANK"])
        world_size = int(os.environ["WORLD_SIZE"])
        local_rank = int(os.environ.get("LOCAL_RANK", rank % max(1, torch.cuda.device_count())))
        device = torch.device(f"cuda:{local_rank}")
        torch.cuda.set_device(device)
        is_main = rank == 0
        return True, rank, world_size, is_main, device

    # Fallback: single-process, single-device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return False, 0, 1, True, device


def time_splits(
    df: pd.DataFrame,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
):
    """
    Time-based split by unique dates. The remainder is test.
    """
    df = df.sort_values(["date", "ticker"]).reset_index(drop=True)
    uniq_dates = sorted(df["date"].dropna().unique())
    n = len(uniq_dates)
    if n < 3:
        raise RuntimeError("Not enough unique dates for train/val/test split.")

    n_train = max(1, int(n * train_ratio))
    n_val = max(1, int(n * val_ratio))
    # Ensure at least 1 date for test
    if n_train + n_val >= n:
        n_val = max(1, n - n_train - 1)

    train_dates = set(uniq_dates[:n_train])
    val_dates = set(uniq_dates[n_train:n_train + n_val])
    test_dates = set(uniq_dates[n_train + n_val:])

    train_df = df[df["date"].isin(train_dates)].copy()
    val_df = df[df["date"].isin(val_dates)].copy()
    test_df = df[df["date"].isin(test_dates)].copy()

    return train_df, val_df, test_df


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--merged_path",
        type=str,
        default="Data/data/merged_lstm_dataset.parquet",
        help="Path to merged dataset produced by merge.py",
    )
    ap.add_argument(
        "--outdir",
        type=str,
        required=True,
        help="Directory to save model checkpoints and metrics.",
    )
    ap.add_argument("--lookback", type=int, default=30)
    ap.add_argument("--epochs", type=int, default=20)
    ap.add_argument("--batch_size", type=int, default=64)
    ap.add_argument("--lr", type=float, default=1e-3)
    ap.add_argument("--weight_decay", type=float, default=1e-4)
    ap.add_argument("--num_workers", type=int, default=4)
    ap.add_argument(
        "--amp",
        action="store_true",
        help="Use mixed-precision training on CUDA.",
    )
    ap.add_argument("--seed", type=int, default=1337)
    ap.add_argument(
        "--dist_mode",
        type=str,
        default="ddp",
        choices=["none", "ddp", "fsdp"],
        help=(
            "Distribution mode: "
            "'none' = single process, "
            "'ddp' = DistributedDataParallel, "
            "'fsdp' = FullyShardedDataParallel."
        ),
    )

    args = ap.parse_args()
    set_seed(args.seed)

    enable_distributed = args.dist_mode in ("ddp", "fsdp")
    is_distributed, rank, world_size, is_main, device = ddp_setup(enable_distributed)

    if is_main:
        print(f"Using device: {device}, "
              f"distributed={is_distributed}, "
              f"world_size={world_size}, "
              f"dist_mode={args.dist_mode}",
              flush=True)

    # ---- load merged dataset ----
    merged_path = args.merged_path
    if not os.path.exists(merged_path):
        raise FileNotFoundError(f"Merged dataset not found: {merged_path}")

    if merged_path.lower().endswith(".parquet"):
        df = pd.read_parquet(merged_path)
    else:
        df = pd.read_csv(merged_path)

    # Ensure date column is datetime
    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.normalize()
    df = df.dropna(subset=["date", "ticker", "target_ret1"]).reset_index(drop=True)

    if is_main:
        print(f"Loaded merged dataset: {df.shape}", flush=True)

    # ---- split data by time ----
    train_df, val_df, test_df = time_splits(df, train_ratio=0.7, val_ratio=0.15)
    if is_main:
        print(
            f"Train: {train_df.shape}, "
            f"Val: {val_df.shape}, "
            f"Test: {test_df.shape}",
            flush=True,
        )

    # ---- config + datasets ----
    cfg = SequenceConfig(lookback=args.lookback, target_col="target_ret1")

    train_ds = NewsPriceSequenceDataset(train_df, cfg)
    val_ds = NewsPriceSequenceDataset(val_df, cfg)
    test_ds = NewsPriceSequenceDataset(test_df, cfg)

    input_dim = train_ds.input_dim
    if is_main:
        print(f"Input dim: {input_dim}, lookback: {cfg.lookback}", flush=True)

    # ---- samplers ----
    if is_distributed:
        train_sampler = DistributedSampler(
            train_ds, num_replicas=world_size, rank=rank, shuffle=True
        )
        val_sampler = DistributedSampler(
            val_ds, num_replicas=world_size, rank=rank, shuffle=False
        )
        test_sampler = DistributedSampler(
            test_ds, num_replicas=world_size, rank=rank, shuffle=False
        )
    else:
        train_sampler = val_sampler = test_sampler = None

    # ---- data loaders ----
    train_loader = DataLoader(
        train_ds,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=args.batch_size,
        shuffle=False,
        sampler=val_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
    )
    test_loader = DataLoader(
        test_ds,
        batch_size=args.batch_size,
        shuffle=False,
        sampler=test_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
    )

    # ---- model ----
    model = LSTMRegressorWithLN(
        input_dim=input_dim,
        hidden_dim=256,
        num_layers=2,
        dropout=0.2,
    ).to(device)

    # Wrap with DDP or FSDP if requested
    if is_distributed and args.dist_mode == "ddp":
        from torch.nn.parallel import DistributedDataParallel as DDP

        model = DDP(
            model,
            device_ids=[device.index] if device.type == "cuda" else None,
            output_device=device.index if device.type == "cuda" else None,
        )
    elif is_distributed and args.dist_mode == "fsdp":
        try:
            from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
        except ImportError as e:
            raise RuntimeError(
                "FSDP requested but not available. "
                "Install a recent PyTorch with torch.distributed.fsdp."
            ) from e

        model = FSDP(model)

    # ---- optimizer & scaler ----
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )
    scaler = GradScaler(enabled=args.amp and device.type == "cuda")
    loss_fn = torch.nn.MSELoss()

    outdir = Path(args.outdir)
    if is_main:
        outdir.mkdir(parents=True, exist_ok=True)
    best_path = outdir / "lstm_best.pt"
    best_val_loss = float("inf")

    # ----------------- training loop -----------------
    for epoch in range(1, args.epochs + 1):
        if is_distributed and train_sampler is not None:
            train_sampler.set_epoch(epoch)

        # ---- train ----
        model.train()
        train_loss_sum = 0.0
        n_train_samples = 0

        for X, y in train_loader:
            X = X.to(device)  # (B, L, D)
            y = y.to(device)  # (B,)

            optimizer.zero_grad(set_to_none=True)

            with autocast(enabled=args.amp and device.type == "cuda"):
                y_pred = model(X)
                loss = loss_fn(y_pred, y)

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()

            train_loss_sum += loss.item() * X.size(0)
            n_train_samples += X.size(0)

        # Reduce train loss across ranks
        if is_distributed:
            t = torch.tensor(
                [train_loss_sum, n_train_samples],
                dtype=torch.float64,
                device=device,
            )
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            train_loss_sum, n_train_samples = t.tolist()

        train_loss = train_loss_sum / max(1, n_train_samples)

        # ---- validation ----
        model.eval()
        se_sum = 0.0  # sum of squared errors
        ae_sum = 0.0  # sum of absolute errors
        n_val_samples = 0

        with torch.no_grad():
            for X, y in val_loader:
                X = X.to(device)
                y = y.to(device)
                with autocast(enabled=args.amp and device.type == "cuda"):
                    y_pred = model(X)

                diff = y_pred - y
                se_sum += (diff ** 2).sum().item()
                ae_sum += diff.abs().sum().item()
                n_val_samples += y.size(0)

        # Reduce validation stats across ranks
        if is_distributed:
            t = torch.tensor(
                [se_sum, ae_sum, n_val_samples],
                dtype=torch.float64,
                device=device,
            )
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            se_sum, ae_sum, n_val_samples = t.tolist()

        val_mse = se_sum / max(1, n_val_samples)
        val_mae = ae_sum / max(1, n_val_samples)
        val_loss = val_mse  # treat MSE as the validation "loss"

        # ---- checkpoint on main rank ----
        if is_main and val_loss < best_val_loss:
            best_val_loss = val_loss
            # unwrap if DDP; FSDP uses its own wrapper but state_dict() works
            if is_distributed and args.dist_mode == "ddp":
                to_save = model.module.state_dict()
            else:
                to_save = model.state_dict()

            torch.save(
                {
                    "model_state_dict": to_save,
                    "input_dim": input_dim,
                    "cfg": cfg.__dict__,
                },
                best_path,
            )

        if is_main:
            print(
                f"[Epoch {epoch:03d}] "
                f"train_loss={train_loss:.6f}  "
                f"val_mse={val_mse:.6f}  "
                f"val_mae={val_mae:.6f}  "
                f"(best_val_mse={best_val_loss:.6f})",
                flush=True,
            )

    # ----------------- evaluate on test set (with best checkpoint) -----------------
    # Load best checkpoint on all ranks
    if best_path.exists():
        ckpt = torch.load(best_path, map_location=device)
        state = ckpt["model_state_dict"]
        if is_distributed and args.dist_mode == "ddp":
            model.module.load_state_dict(state)
        else:
            model.load_state_dict(state)
        if is_main:
            print(f"Loaded best checkpoint from {best_path}", flush=True)

    model.eval()
    se_sum = 0.0
    ae_sum = 0.0
    n_test_samples = 0

    with torch.no_grad():
        for X, y in test_loader:
            X = X.to(device)
            y = y.to(device)
            with autocast(enabled=args.amp and device.type == "cuda"):
                y_pred = model(X)

            diff = y_pred - y
            se_sum += (diff ** 2).sum().item()
            ae_sum += diff.abs().sum().item()
            n_test_samples += y.size(0)

    if is_distributed:
        t = torch.tensor(
            [se_sum, ae_sum, n_test_samples],
            dtype=torch.float64,
            device=device,
        )
        dist.all_reduce(t, op=dist.ReduceOp.SUM)
        se_sum, ae_sum, n_test_samples = t.tolist()

    test_mse = se_sum / max(1, n_test_samples)
    test_mae = ae_sum / max(1, n_test_samples)

    # ---- save metrics on main rank ----
    if is_main:
        print(f"Test MSE={test_mse:.6f}, Test MAE={test_mae:.6f}", flush=True)

        metrics = {
            "val_best_mse": best_val_loss,
            "test_mse": test_mse,
            "test_mae": test_mae,
            "world_size": world_size,
            "dist_mode": args.dist_mode,
        }
        outdir.mkdir(parents=True, exist_ok=True)
        metrics_path = outdir / "metrics.csv"
        pd.DataFrame([metrics]).to_csv(metrics_path, index=False)
        print(f"Saved metrics to {metrics_path}", flush=True)

    # clean up process group
    if is_distributed:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()
