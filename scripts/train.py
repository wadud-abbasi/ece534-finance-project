#!/usr/bin/env python
"""
train.py

Train a 2-layer LSTM + LayerNorm + MLP on the merged dataset generated by merge.py,
with support for:
  - Data parallelism via DistributedDataParallel (DDP)
  - Optional model sharding via FullyShardedDataParallel (FSDP)

Memory-friendly version:
  - Reads only a subset of FinBERT embeddings (default: emb_0..emb_127)
  - Keeps only the columns actually used for modeling
  - Stores numerics as float16 in the DataFrame to reduce RAM
  - Optional ticker cap via --max_tickers

Example (single node, 1 GPU, no distributed):

  python scripts/train.py \
    --merged_path data/data/merged_lstm_dataset_half.parquet \
    --outdir outputs/lstm_1g_1n \
    --lookback 30 \
    --epochs 20 \
    --batch_size 64 \
    --dist_mode none \
    --amp

Example (single node, 4 GPUs, DDP):

  torchrun --nproc_per_node=4 scripts/train.py \
    --merged_path data/data/merged_lstm_dataset_half.parquet \
    --outdir outputs/lstm_ddp_4g \
    --lookback 30 \
    --epochs 20 \
    --batch_size 64 \
    --dist_mode ddp \
    --amp
"""

import argparse
import os
import random
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import GradScaler, autocast

from model import SequenceConfig, NewsPriceSequenceDataset, LSTMRegressorWithLN


# ----------------- utilities -----------------


def set_seed(s: int = 1337):
    random.seed(s)
    np.random.seed(s)
    torch.manual_seed(s)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(s)


def ddp_setup(enable_distributed: bool):
    """
    Initialize torch.distributed if launched under torchrun OR Slurm (srun),
    when distributed is enabled.

    Returns:
        is_distributed: bool
        rank: int
        world_size: int
        is_main: bool
        device: torch.device
    """
    if enable_distributed:
        # Try to read standard env vars first; fall back to Slurm.
        rank = int(os.environ.get("RANK", os.environ.get("SLURM_PROCID", 0)))
        world_size = int(os.environ.get("WORLD_SIZE", os.environ.get("SLURM_NTASKS", 1)))
        local_rank = int(os.environ.get("LOCAL_RANK", os.environ.get("SLURM_LOCALID", rank)))

        if world_size > 1:
            # MASTER_ADDR / MASTER_PORT should be set in the sbatch script.
            dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)

            if torch.cuda.is_available():
                device = torch.device(f"cuda:{local_rank}")
                torch.cuda.set_device(device)
            else:
                device = torch.device("cpu")

            is_main = rank == 0
            return True, rank, world_size, is_main, device

    # Fallback: single-process, single-device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    return False, 0, 1, True, device


def time_splits(
    df: pd.DataFrame,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
):
    """
    Time-based split by unique dates. The remainder is test.
    """
    df = df.sort_values(["date", "ticker"]).reset_index(drop=True)
    uniq_dates = sorted(df["date"].dropna().unique())
    n = len(uniq_dates)
    if n < 3:
        raise RuntimeError("Not enough unique dates for train/val/test split.")

    n_train = max(1, int(n * train_ratio))
    n_val = max(1, int(n * val_ratio))
    # Ensure at least 1 date for test
    if n_train + n_val >= n:
        n_val = max(1, n - n_train - 1)

    train_dates = set(uniq_dates[:n_train])
    val_dates = set(uniq_dates[n_train:n_train + n_val])
    test_dates = set(uniq_dates[n_train + n_val:])

    train_df = df[df["date"].isin(train_dates)].copy()
    val_df = df[df["date"].isin(val_dates)].copy()
    test_df = df[df["date"].isin(test_dates)].copy()

    return train_df, val_df, test_df


# ----------------- main -----------------


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--merged_path",
        type=str,
        default="data/data/merged_lstm_dataset.parquet",
        help="Path to merged dataset produced by merge.py",
    )
    ap.add_argument(
        "--outdir",
        type=str,
        required=True,
        help="Directory to save model checkpoints and metrics.",
    )
    ap.add_argument("--lookback", type=int, default=30)
    ap.add_argument("--epochs", type=int, default=20)
    ap.add_argument("--batch_size", type=int, default=64)
    ap.add_argument("--lr", type=float, default=1e-3)
    ap.add_argument("--weight_decay", type=float, default=1e-4)
    ap.add_argument("--num_workers", type=int, default=4)
    ap.add_argument(
        "--amp",
        action="store_true",
        help="Use mixed-precision training on CUDA.",
    )
    ap.add_argument("--seed", type=int, default=1337)
    ap.add_argument(
        "--dist_mode",
        type=str,
        default="none",
        choices=["none", "ddp", "fsdp"],
        help=(
            "Distribution mode: "
            "'none' = single process, "
            "'ddp' = DistributedDataParallel, "
            "'fsdp' = FullyShardedDataParallel."
        ),
    )
    ap.add_argument(
        "--emb_dim_cap",
        type=int,
        default=128,
        help="How many FinBERT embedding dimensions to keep (use emb_0..emb_{cap-1}).",
    )
    ap.add_argument(
        "--max_tickers",
        type=int,
        default=0,
        help="If >0, limit to this many tickers (useful for debugging / memory).",
    )
    ap.add_argument(
        "--save_every",
        type=int,
        default=0,
        help="If >0, save a checkpoint every this many epochs (on main rank).",
    )


    args = ap.parse_args()
    set_seed(args.seed)

    enable_distributed = args.dist_mode in ("ddp", "fsdp")
    is_distributed, rank, world_size, is_main, device = ddp_setup(enable_distributed)

    if is_main:
        print(
            f"Using device: {device}, "
            f"distributed={is_distributed}, "
            f"world_size={world_size}, "
            f"dist_mode={args.dist_mode}",
            flush=True,
        )

    # ---- load merged dataset (column-limited, memory-friendly) ----
    merged_path = args.merged_path
    if not os.path.exists(merged_path):
        raise FileNotFoundError(f"Merged dataset not found: {merged_path}")

    # Define which columns we WANT to keep
    emb_dim_cap = args.emb_dim_cap
    emb_cols = [f"emb_{i}" for i in range(emb_dim_cap)]

    base_cols = [
        "ret0",
        "sent_neg",
        "sent_neu",
        "sent_pos",
        "ret_djia",
        "ret_nasdaqcom",
        "ret_sp500",
    ]

    keep_cols = ["date", "ticker", "target_ret1"] + emb_cols + base_cols

    if merged_path.lower().endswith(".parquet"):
        # Read only the columns we intend to use
        df = pd.read_parquet(merged_path, columns=keep_cols)
    else:
        # CSV fallback: read everything, then slice (less memory-friendly)
        df = pd.read_csv(merged_path)
        # Only keep the columns that exist in the frame
        existing = [c for c in keep_cols if c in df.columns]
        df = df[existing].copy()
        # If some expected columns are missing, we can fill them with 0.0
        missing = [c for c in keep_cols if c not in df.columns]
        for c in missing:
            if c == "date" or c == "ticker":
                continue
            df[c] = 0.0

    # Ensure date column is datetime
    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.normalize()
    df = df.dropna(subset=["date", "ticker", "target_ret1"]).reset_index(drop=True)

    # Optional: limit to a subset of tickers to further reduce memory
    if args.max_tickers > 0:
        uniq_tickers = sorted(df["ticker"].unique())
        keep_tickers = set(uniq_tickers[: args.max_tickers])
        df = df[df["ticker"].isin(keep_tickers)].reset_index(drop=True)
        if is_main:
            print(
                f"Applied ticker cap: max_tickers={args.max_tickers}, "
                f"resulting shape={df.shape}",
                flush=True,
            )

    # Downcast numeric columns to float16 to save RAM
    numeric_cols = [c for c in df.columns if c not in ("date", "ticker")]
    for c in numeric_cols:
        df[c] = df[c].astype(np.float16)

    if is_main:
        print(f"Using columns: {list(df.columns)}", flush=True)
        print(f"Loaded merged dataset (trimmed): {df.shape}", flush=True)

    # ---- split data by time ----
    train_df, val_df, test_df = time_splits(df, train_ratio=0.7, val_ratio=0.15)
    if is_main:
        print(
            f"Train: {train_df.shape}, "
            f"Val: {val_df.shape}, "
            f"Test: {test_df.shape}",
            flush=True,
        )

    # ---- config + datasets ----
    cfg = SequenceConfig(lookback=args.lookback, target_col="target_ret1")

    train_ds = NewsPriceSequenceDataset(train_df, cfg)
    val_ds = NewsPriceSequenceDataset(val_df, cfg)
    test_ds = NewsPriceSequenceDataset(test_df, cfg)

    input_dim = train_ds.input_dim
    if is_main:
        print(f"Input dim: {input_dim}, lookback: {cfg.lookback}", flush=True)

    # ---- samplers ----
    if is_distributed:
        train_sampler = DistributedSampler(
            train_ds, num_replicas=world_size, rank=rank, shuffle=True
        )
        val_sampler = DistributedSampler(
            val_ds, num_replicas=world_size, rank=rank, shuffle=False
        )
        test_sampler = DistributedSampler(
            test_ds, num_replicas=world_size, rank=rank, shuffle=False
        )
    else:
        train_sampler = val_sampler = test_sampler = None

    # ---- data loaders ----
    train_loader = DataLoader(
        train_ds,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
        drop_last=True,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=args.batch_size,
        shuffle=False,
        sampler=val_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
    )
    test_loader = DataLoader(
        test_ds,
        batch_size=args.batch_size,
        shuffle=False,
        sampler=test_sampler,
        num_workers=args.num_workers,
        pin_memory=True,
    )

    # ---- model ----
    model = LSTMRegressorWithLN(
        input_dim=input_dim,
        hidden_dim=256,
        num_layers=2,
        dropout=0.2,
    ).to(device)

    # Wrap with DDP or FSDP if requested
    if is_distributed and args.dist_mode == "ddp":
        from torch.nn.parallel import DistributedDataParallel as DDP

        model = DDP(
            model,
            device_ids=[device.index] if device.type == "cuda" else None,
            output_device=device.index if device.type == "cuda" else None,
        )
    elif is_distributed and args.dist_mode == "fsdp":
        try:
            from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
        except ImportError as e:
            raise RuntimeError(
                "FSDP requested but not available. "
                "Install a recent PyTorch with torch.distributed.fsdp."
            ) from e

        model = FSDP(model)

    # ---- optimizer & scaler ----
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )
    scaler = GradScaler(enabled=args.amp and device.type == "cuda")
    loss_fn = torch.nn.MSELoss()

    outdir = Path(args.outdir)
    if is_main:
        outdir.mkdir(parents=True, exist_ok=True)

    best_path = outdir / "lstm_best.pt"
    best_val_loss = float("inf")

    # per-epoch history for curves
    history_path = outdir / "training_history.csv"
    if is_main and history_path.exists():
        history_path.unlink()  # start fresh each run


    for epoch in range(1, args.epochs + 1):
        if is_distributed and train_sampler is not None:
            train_sampler.set_epoch(epoch)   # âœ… correct

        # ---- train ----
        model.train()
        train_loss_sum = 0.0
        n_train_samples = 0

        for X, y in train_loader:
            X = X.to(device)  # (B, L, D)
            y = y.to(device)  # (B,)

            optimizer.zero_grad(set_to_none=True)

            with autocast(enabled=args.amp and device.type == "cuda"):
                y_pred = model(X)
                loss = loss_fn(y_pred, y)

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()

            train_loss_sum += loss.item() * X.size(0)
            n_train_samples += X.size(0)

        # Reduce train loss across ranks
        if is_distributed:
            t = torch.tensor(
                [train_loss_sum, n_train_samples],
                dtype=torch.float64,
                device=device,
            )
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            train_loss_sum, n_train_samples = t.tolist()

        train_loss = train_loss_sum / max(1, n_train_samples)

        # ---- validation ----
        model.eval()
        se_sum = 0.0  # sum of squared errors
        ae_sum = 0.0  # sum of absolute errors
        n_val_samples = 0

        with torch.no_grad():
            for X, y in val_loader:
                X = X.to(device)
                y = y.to(device)
                with autocast(enabled=args.amp and device.type == "cuda"):
                    y_pred = model(X)

                diff = y_pred - y
                se_sum += (diff ** 2).sum().item()
                ae_sum += diff.abs().sum().item()
                n_val_samples += y.size(0)

        # Reduce validation stats across ranks
        if is_distributed:
            t = torch.tensor(
                [se_sum, ae_sum, n_val_samples],
                dtype=torch.float64,
                device=device,
            )
            dist.all_reduce(t, op=dist.ReduceOp.SUM)
            se_sum, ae_sum, n_val_samples = t.tolist()

        val_mse = se_sum / max(1, n_val_samples)
        val_mae = ae_sum / max(1, n_val_samples)
        val_loss = val_mse  # treat MSE as the validation "loss"

        # ---- log per-epoch metrics (for curves) on main rank ----
        if is_main:
            row = {
                "epoch": epoch,
                "train_loss": float(train_loss),
                "val_mse": float(val_mse),
                "val_mae": float(val_mae),
            }
            # append one row per epoch, so if job dies you still have partial curves
            write_header = not history_path.exists()
            pd.DataFrame([row]).to_csv(
                history_path,
                mode="a",
                header=write_header,
                index=False,
            )


        # ---- periodic checkpoint on main rank ----
        if is_main and args.save_every > 0 and epoch % args.save_every == 0:
            if is_distributed and args.dist_mode == "ddp":
                periodic_state = model.module.state_dict()
            else:
                periodic_state = model.state_dict()

            ckpt_path = outdir / f"checkpoint_epoch_{epoch:03d}.pt"
            torch.save(
                {
                    "model_state_dict": periodic_state,
                    "input_dim": input_dim,
                    "cfg": cfg.__dict__,
                    "epoch": epoch,
                },
                ckpt_path,
            )

        # ---- best checkpoint on main rank ----
        if is_main and val_loss < best_val_loss:
            best_val_loss = val_loss
            if is_distributed and args.dist_mode == "ddp":
                to_save = model.module.state_dict()
            else:
                to_save = model.state_dict()

            torch.save(
                {
                    "model_state_dict": to_save,
                    "input_dim": input_dim,
                    "cfg": cfg.__dict__,
                    "epoch": epoch,
                },
                best_path,
            )

        if is_main:
            print(
                f"[Epoch {epoch:03d}] "
                f"train_loss={train_loss:.6f}  "
                f"val_mse={val_mse:.6f}  "
                f"val_mae={val_mae:.6f}  "
                f"(best_val_mse={best_val_loss:.6f})",
                flush=True,
            )

    # ----------------- evaluate on test set (with best checkpoint) -----------------
    # Load best checkpoint on all ranks
    if best_path.exists():
        ckpt = torch.load(best_path, map_location=device)
        state = ckpt["model_state_dict"]
        if is_distributed and args.dist_mode == "ddp":
            model.module.load_state_dict(state)
        else:
            model.load_state_dict(state)
        if is_main:
            print(f"Loaded best checkpoint from {best_path}", flush=True)

    model.eval()
    se_sum = 0.0
    ae_sum = 0.0
    n_test_samples = 0

    with torch.no_grad():
        for X, y in test_loader:
            X = X.to(device)
            y = y.to(device)
            with autocast(enabled=args.amp and device.type == "cuda"):
                y_pred = model(X)

            diff = y_pred - y
            se_sum += (diff ** 2).sum().item()
            ae_sum += diff.abs().sum().item()
            n_test_samples += y.size(0)

    if is_distributed:
        t = torch.tensor(
            [se_sum, ae_sum, n_test_samples],
            dtype=torch.float64,
            device=device,
        )
        dist.all_reduce(t, op=dist.ReduceOp.SUM)
        se_sum, ae_sum, n_test_samples = t.tolist()

    test_mse = se_sum / max(1, n_test_samples)
    test_mae = ae_sum / max(1, n_test_samples)

    # ---- save metrics on main rank ----
    if is_main:
        print(f"Test MSE={test_mse:.6f}, Test MAE={test_mae:.6f}", flush=True)

        metrics = {
            "val_best_mse": best_val_loss,
            "test_mse": test_mse,
            "test_mae": test_mae,
            "world_size": world_size,
            "dist_mode": args.dist_mode,
            "emb_dim_cap": emb_dim_cap,
            "max_tickers": args.max_tickers,
        }
        outdir.mkdir(parents=True, exist_ok=True)
        metrics_path = outdir / "metrics.csv"
        pd.DataFrame([metrics]).to_csv(metrics_path, index=False)
        print(f"Saved metrics to {metrics_path}", flush=True)

    # clean up process group
    if is_distributed:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()
